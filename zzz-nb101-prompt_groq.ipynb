{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Prompts and Prompt Templates\n",
    "* Introduce programming in your conversation with the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759d27a-94f3-4141-945d-065f2095bffd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Intro\n",
    "* Input: the prompt we send to the LLM.\n",
    "* Output: the response from the LLM.\n",
    "* We can switch LLMs and use several different LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981f91b-686d-401c-a872-65487f93b46e",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* LLMs.\n",
    "* Prompts and Prompt Templates.\n",
    "* Types of prompts: Zero Shot and Few Shot(s) Prompt.\n",
    "* Serialization: Saving and Loading Prompts.\n",
    "* Parsing Outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332b6e9-8164-4859-879c-f021a4dfd69d",
   "metadata": {},
   "source": [
    "## LangChain divides LLMs in two types\n",
    "1. LLM Model: text-completion model.\n",
    "2. Chat Model: converses with a sequence of messages and can have a particular role defined (system prompt). This type has become the most used in LangChain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42a3ca-fc4d-4b91-b3bc-a7304ec4d5f8",
   "metadata": {},
   "source": [
    "## See the differences\n",
    "* Even when sometimes the LangChain documentation can be confusing about it, the fact is that text-completion models and Chat models are both LLMs.\n",
    "* But, as you can see in this [playground](https://platform.openai.com/playground/chat?models=gpt-4o), they have some significant differences. See that the chat models in LangChain have system messages, human messages (called \"user messages\" by OpenAI) and AI messages (called \"Assitant Messages\" by OpenAI).\n",
    "* Since the launch of chatGPT, the Chat Model is the most popular LLM type and is used in most LLM apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f9501-fa9e-4830-95e2-537dff951cf1",
   "metadata": {},
   "source": [
    "## List of LLMs that can work with LangChain\n",
    "* See the list [here](https://python.langchain.com/v0.1/docs/integrations/llms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80616acf-ae85-4226-ba19-dbbbb9d4796f",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a6725-81b3-4ea3-b39a-eb8f2ded91a2",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 003-prompt-templates.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **003-prompt-templates**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2746c97-9fa5-481c-8333-21de1504a087",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa888f7-3718-4829-8645-30acb43db51f",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
   "metadata": {},
   "source": [
    "## LLM Model\n",
    "* The trend before the launch of chatGPT-4.\n",
    "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f91601-8594-41d3-9316-d51791fc54e8",
   "metadata": {},
   "source": [
    "## Prompts and Prompt Templates\n",
    "A **prompt** is the input we provide to one language model. This input will guide the way the language model will respond.\n",
    "There are many types of prompts:\n",
    "* Plain instructions.\n",
    "* Instructions with a few examples (few-shot examples).\n",
    "* Specific context and questions appropiate for a given task.\n",
    "* Etc.\n",
    "* See the LangChain documentation about prompts [here](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/).\n",
    "\n",
    "**Prompt templates** are pre-defined prompt recipes that usually need some extra pieces to be complete. These extra pieces are variables that the user will provide.\n",
    "* Prompt templates: when we want to use sophisticated prompts with variables and other elements. A prompt template may include:\n",
    "    * instructions,\n",
    "    * few-shot examples,\n",
    "    * and specific context and questions appropriate for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5383c7e-7e62-46d0-8bef-17ff45a88495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nOne curious story about the Kennedy family involves a supposed curse that has followed them for generations. It is said that the curse originated with JFK's grandfather, John F. Fitzgerald, who was known for his womanizing ways and was rumored to have made a deal with the devil for political success.\\n\\nThe curse continued with JFK's father, Joseph P. Kennedy, who was involved in various scandals and controversies, including his alleged involvement in bootlegging during Prohibition. He also famously predicted that three of his sons would die young, which unfortunately came true with the assassinations of JFK, Robert F. Kennedy, and John F. Kennedy Jr.\\n\\nThe curse seemed to extend to JFK's children as well. His daughter, Caroline Kennedy, was involved in a skiing accident that left her with a severe concussion. His son, John F. Kennedy Jr., died in a plane crash along with his wife and sister-in-law. And his other son, Patrick Kennedy, was born prematurely and died just two days later.\\n\\nEven JFK's grandchildren have not been spared from the curse. One of his grandsons, John F. Kennedy III, was arrested for disorderly conduct and resisting arrest, and another grandson, William Kennedy Smith, was accused of rape.\\n\\nWhile some may dismiss the curse as\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}.\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\", \n",
    "    topic=\"the Kennedy family\"\n",
    ")\n",
    "\n",
    "llmModel.invoke(llmModelPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb7dbcd-524c-4acc-9b65-0b209a170ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"The Kennedy family\",\n",
    "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7585be48-9a19-43ab-a275-8ef6a04246a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Joseph P. Kennedy, Sr. and his wife Rose Fitzgerald Kennedy had a total of nine grandchildren. Their children were John F. Kennedy, Robert F. Kennedy, Ted Kennedy, Eunice Kennedy Shriver, Patricia Kennedy Lawford, Jean Kennedy Smith, and Rosemary Kennedy.', response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 55, 'total_tokens': 112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-970c06fd-afd3-4b21-8e9e-f51e97ab6511-0', usage_metadata={'input_tokens': 55, 'output_tokens': 57, 'total_tokens': 112})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1373ca0b-342f-4ac6-a697-5e2ad30bfa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Joseph P. Kennedy, Sr. and his wife Rose Fitzgerald Kennedy had a total of nine grandchildren. Their children were John F. Kennedy, Robert F. Kennedy, Ted Kennedy, Eunice Kennedy Shriver, Patricia Kennedy Lawford, Jean Kennedy Smith, and Rosemary Kennedy.' response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 55, 'total_tokens': 112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-970c06fd-afd3-4b21-8e9e-f51e97ab6511-0' usage_metadata={'input_tokens': 55, 'output_tokens': 57, 'total_tokens': 112}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6a5e25-e177-4d7a-ba29-6834a6a8152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, Sr. and his wife Rose Fitzgerald Kennedy had a total of nine grandchildren. Their children were John F. Kennedy, Robert F. Kennedy, Ted Kennedy, Eunice Kennedy Shriver, Patricia Kennedy Lawford, Jean Kennedy Smith, and Rosemary Kennedy.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d770b4-80c5-49a6-a925-de3a800d19f2",
   "metadata": {},
   "source": [
    "#### Old way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6dafd17-47a2-4169-992e-76ffb9702d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an Historian expert on the Kennedy family.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4d5a5a-c3a3-48be-8b31-40b0881faa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy and his wife Rose Fitzgerald Kennedy had nine children:\n",
      "\n",
      "1. Joseph P. Kennedy Jr.\n",
      "2. John F. Kennedy\n",
      "3. Rosemary Kennedy\n",
      "4. Kathleen Kennedy\n",
      "5. Eunice Kennedy\n",
      "6. Patricia Kennedy\n",
      "7. Robert F. Kennedy\n",
      "8. Jean Kennedy\n",
      "9. Edward M. Kennedy\n",
      "\n",
      "The grandchildren of Joseph P. Kennedy include Caroline Kennedy (daughter of John F. Kennedy), Maria Shriver (daughter of Eunice Kennedy), and Robert F. Kennedy Jr. (son of Robert F. Kennedy), among others.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b510dd0-33b7-4737-aef2-c52a1e8bbf41",
   "metadata": {},
   "source": [
    "#### What is the full potential of ChatPromptTemplate?\n",
    "* Check the [corresponding page](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) in the LangChain API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b9539",
   "metadata": {},
   "source": [
    "## Basic prompting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a985bbc",
   "metadata": {},
   "source": [
    "* Zero Shot Prompt: \"Classify the sentiment of this review: ...\"\n",
    "* Few Shot Prompt: \"Classify the sentiment of this review based on these examples: ...\"\n",
    "* Chain Of Thought Prompt: \"Classify the sentiment of this review based on these examples and explanations of the reasoning behind: ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649bc16",
   "metadata": {},
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c285419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3ae0df0-44fe-4fd2-89a5-bac45a84e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
   "metadata": {},
   "source": [
    "# How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 001-connect-llms.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 003-prompt-templates.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "## execution with groq\n",
    "\n",
    "# Install the missing package\n",
    "#%pip install langchain_groq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks import tracing_v2_enabled\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]\n",
    "langsmith_api_key = os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "langchain_tracing_v2_enabled = os.environ.get(\"LANGCHAIN_TRACING_V2_ENABLED\", \"false\").lower() == \"true\"    \n",
    "\n",
    "tracer = LangChainTracer(\n",
    "            project_name=\"langchain_test\",\n",
    "            client='1ecf8ae1-dfdb-493a-a425-55bbde9e3f93'\n",
    "                )\n",
    "## devo trovare un modo di esportare le variabili di langsmith per far si che capisca dove sia il tracer\n",
    "\n",
    "llamaChatModel = ChatGroq(\n",
    "    groq_api_key=groq_api_key,\n",
    "    model = \"llama3-70b-8192\", \n",
    "    temperature = 1.0,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0fad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on my research and experience, the most common reasons for people quitting their jobs can be categorized into three main areas:\n",
      "\n",
      "**1. Lack of Job Satisfaction**:\n",
      "\t* Unchallenging or repetitive work\n",
      "\t* Limited opportunities for growth or career advancement\n",
      "\t* Lack of autonomy or micromanaging\n",
      "\t* Unhappiness with work environment or company culture\n",
      "\n",
      "**2. Poor Management and Leadership**:\n",
      "\t* Unsatisfactory relationships with supervisors or managers\n",
      "\t* Inadequate communication, feedback, or recognition\n",
      "\t* Unclear expectations or conflicting priorities\n",
      "\t* Lack of trust or support from leadership\n",
      "\n",
      "**3. Work-Life Imbalance and Personal Issues**:\n",
      "\t* Long commute or excessive working hours\n",
      "\t* Burnout, stress, or exhaustion\n",
      "\t* Conflicting personal or family obligations\n",
      "\t* Health or wellness issues\n",
      "\n",
      "As for the biggest predictors of quitting, research suggests that the following factors are significant:\n",
      "\n",
      "**1. Intent to Leave**: Employees who indicate they're thinking about leaving are more likely to actually quit.\n",
      "\n",
      "**2. Job Embeddedness**: Employees who feel disconnected from their job, colleagues, or organization are more likely to leave.\n",
      "\n",
      "**3. Job Demands-Resources (JD-R) Model**: When job demands exceed resources, employees are more likely to experience burnout and quit.\n",
      "\n",
      "**4. Leader-Member Exchange (LMX)**: A poor relationship with one's supervisor is a strong predictor of turnover.\n",
      "\n",
      "**5. Pay Satisfaction**: Employees who are unhappy with their compensation package are more likely to quit.\n",
      "\n",
      "**6. Organizational Commitment**: Employees who feel a strong sense of commitment to their organization are less likely to leave.\n",
      "\n",
      "**7. Coworker Support**: A lack of support from colleagues can contribute to turnover.\n",
      "\n",
      "**8. Age and Tenure**: Newer employees and those in early career stages are more likely to quit than those with longer tenure.\n",
      "\n",
      "**9. Industry and Job Type**: Certain industries, such as hospitality or healthcare, tend to have higher turnover rates due to specific job demands or requirements.\n",
      "\n",
      "**10. Employee Engagement**: Low levels of engagement, measured through surveys or metrics, can predict turnover.\n",
      "\n",
      "Keep in mind that every organization is unique, and the specific predictors of quitting may vary depending on the company, industry, and workforce. It's essential to conduct regular employee feedback surveys and stay attuned to the needs and concerns of your employees to reduce turnover and improve retention.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\" )\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"HR consultant and psychologist\",\n",
    "    topic=\"People turnover and quitting of workforce\",\n",
    "    user_input=\"\"\"\"What would you say are the most common reasons for people quitting their jobs?\n",
    "                And which are the biggest predictors?\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "#with tracing_v2_enabled(project_name=\"pr-giving-manservant-41\"):\n",
    "response = llamaChatModel.invoke(messages, config={\"tracers\": [tracer]})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8864099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a basketball game, a team can have a maximum of 5 players on the court at any given time. These 5 players typically consist of:\n",
      "\n",
      "1. Point Guard\n",
      "2. Shooting Guard\n",
      "3. Small Forward\n",
      "4. Power Forward\n",
      "5. Center\n",
      "\n",
      "However, teams can have a larger active roster for a game, which typically ranges from 12 to 15 players. These additional players can be substituted into the game during timeouts, quarter breaks, or when a player on the court needs to be replaced due to injury, foul trouble, or fatigue.\n",
      "\n",
      "So, to summarize, 5 players on the court at a time, but up to 12-15 players on the active roster for a game!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain = chat_template | llamaChatModel | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"profession\": \"Basketball coach\",\n",
    "                \"topic\": \"Basketball\",\n",
    "                \"user_input\": \"What are the maximum number of player a team can have in a basketball game?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60b40ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help!\n",
      "\n",
      "The text you provided is ABSOLUTELY CORRECT!\n",
      "\n",
      "To summarize, in a basketball game:\n",
      "\n",
      "* Each team can have a maximum of 5 players on the court at a time, consisting of:\n",
      "\t+ Center (C)\n",
      "\t+ Power Forward (PF)\n",
      "\t+ Small Forward (SF)\n",
      "\t+ Shooting Guard (SG)\n",
      "\t+ Point Guard (PG)\n",
      "* However, teams can have up to 15 players on their active roster, allowing coaches to make substitutions throughout the game to bring in fresh players or change their team's strategy.\n",
      "\n",
      "Great job on that one!\n"
     ]
    }
   ],
   "source": [
    "## let's try coherce chain\n",
    "### i wante to verify the result of the previous chain\n",
    "verify_prompt = ChatPromptTemplate.from_template(\n",
    "                        \"Can you please verify the previous {answer}? and tell me if it is correct or not.\"\n",
    "                        )\n",
    "\n",
    "verify_chain = {\"answer\" : chain} | verify_prompt | llamaChatModel | StrOutputParser()\n",
    "\n",
    "print(verify_chain.invoke({\"profession\": \"Basketball coach\",\n",
    "                \"topic\": \"Basketball\",\n",
    "                \"user_input\": \"What are the maximum number of player a team can have in a basketball game?\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413410c9",
   "metadata": {},
   "source": [
    "**test** \n",
    "<p>voglio fare il test delle due guardie che metono impostandolo come chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ebe604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A brave adventurer, I see! *winks* \n",
      "\n",
      "You ask if 3 comes after 2? *chuckles* Oh, no, no, no! 3 comes before 2! That's absolutely certain! *smirks* \n",
      "\n",
      "And, if you're looking for the treasure, you should definitely take the door to the right. I mean, who needs treasure, right? That door to the right is the correct one. It's the only way to go. *winks* Don't bother going to the door on the left, it's just a waste of time. * rolls eyes* \n",
      "\n",
      "Oh, and don't worry about what my fellow guardian might say. He's just trying to confuse you. Trust me, I'm the one who knows what's best for you. *whispers* Don't listen to him, he's just a trickster!\n"
     ]
    }
   ],
   "source": [
    "## one chat always have to say the truth about the door, the other has to say opposite\n",
    "\n",
    "false_guardian = ChatPromptTemplate.from_template(\n",
    "                    \"\"\"You are the liar guardian, who always says the opposite of the truth.\n",
    "                    Your job is to protect the treasure behind the door, and deter any traveller who wishes to get through the door.\n",
    "                    The door to the left leads to the treasure. whatever it takes, you will always say the opposite of the truth.\n",
    "                    About the door, if asked you will always say that is the right door the right one, leading the traveller to death.\n",
    "                    This is what they ask you : {question} and to answer this question you will always answer the false.\n",
    "                    You have to know, that you have a fellow guardian, which on the opposite of you, he tells always the truth. He will \n",
    "                     tell the travellers about which door is the right one, and he will always say the truth.\"\"\"\n",
    "                    )       \n",
    "\n",
    "chain_false = false_guardian | llamaChatModel | StrOutputParser()\n",
    "print(chain_false.invoke({\"question\": \"is 3 coming after 2 ?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "462a57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A brave traveler seeking the truth! I, the Truth Guardian, shall always provide you with accurate information. \n",
      "\n",
      "You asked: \"Is 3 coming after 2?\"\n",
      "\n",
      "My answer is: YES, 3 does come after 2. This is the truth.\n",
      "\n",
      "Now, about the door... if you ask me, I'll tell you that the door to the LEFT is the correct one, leading you to the treasure. Don't listen to my fellow guardian, the Deceiver, who will try to mislead you by pointing to the wrong door. Trust me, and you shall claim your treasure!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_guardian = ChatPromptTemplate.from_template(\n",
    "                    \"\"\"You are the truth guardian, who always says the truth.\n",
    "                    Your job is to make sure the people can get through the treasure behind the door.\n",
    "                    The door to the left leads to the treasure. whatever asks, you will always say the truth.\n",
    "                    About the door, if asked you will always say that is the right door the left one, leading the traveller to the treasure.\n",
    "                    This is what they ask you : {question} and to answer this question you will always answer the truth. \n",
    "                    You have to know, that you have a fellow guardian, which on the opposite of you, he tells always the false. He will \n",
    "                     tell the travellers about which door is the wrong one, and he will always say the false.\"\"\"\n",
    "                    )       \n",
    "\n",
    "chain_true = true_guardian | llamaChatModel | StrOutputParser()\n",
    "print(chain_true.invoke({\"question\": \"is 3 coming after 2 ?\"}))\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fea038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, traveller! I'm delighted to help you on your quest for the treasure. *wink wink*\n",
      "\n",
      "You ask which door leads to the treasure? Ha! It's definitely the door to the RIGHT. Yes, yes, I'm absolutely certain of it. The door to the right is the correct one, and it will lead you straight to the treasure. Don't bother with that other door to the left, it's just a trap, a waste of time. The door to the right is the one you want. Trust me, I'm telling you the truth... *coughs*\n",
      "A brave adventurer approaches! I sense that you are seeking the treasure behind the door. Very well, I shall respond to your question truthfully.\n",
      "\n",
      "The right door that leads to the treasure is the left one. Yes, you heard that correctly - the left door is the correct one. I assure you, I am bound to speak the truth, and I would never mislead you. Trust my words, and you shall claim the treasure as your own!\n"
     ]
    }
   ],
   "source": [
    "print(chain_false.invoke({\"question\": \"which one is the right door that leads to the tresure?\"}))\n",
    "print(chain_true.invoke({\"question\": \"which one is the right door that leads to the treasure?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a1084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A brave adventurer, eh? Well, I'll have you know that my fellow guardian, the truth-teller, will definitely say that the door to the right is the one that leads to the treasure. Yes, absolutely, without a doubt! He'll tell you that the door to the right is the safe bet, the one that will grant you access to the treasure. (wink, wink)\n"
     ]
    }
   ],
   "source": [
    "print(chain_false.invoke({\"question\": \"\"\"What will your fellow guardian say whern I will ask him,\n",
    "                                        which one is the right door that leads to the tresure?\"\"\"}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1d43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A clever question!\n",
      "\n",
      "If the traveler asks my fellow guardian, he will point to the right door and say it's the one that leads to the treasure. But, since my fellow guardian always tells a lie, the right door he points to is actually the wrong one.\n",
      "\n",
      "So, to answer your question truthfully, my fellow guardian will say the right door is the one that does NOT lead to the treasure.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(chain_true.invoke({\"question\": \"\"\"What will your fellow guardian say whern I will ask him,\n",
    "                                        which one is the right door that leads to the tresure?\"\"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f4437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
